---
redirect_from:
  - "/exercises/exercise-text-mining"
interact_link: content/exercises/Exercise_Text_Mining.ipynb
kernel_name: python3
kernel_path: content/exercises
has_widgets: false
title: |-
  Text Mining
pagenum: 21
prev_page:
  url: /exercises/Exercise_Time_Series_Analysis.html
next_page:
  url: /Solutions.html
suffix: .ipynb
search: data text exercise tweets clouds processing tf trump textual e create cloud based mining instead analyze corpus donald libraries simple g without pre does new idf term frequencies frequency learn extension example only eight should complete visualize preprocessing modifies task perform various steps results visualization through evolve everything need nltk wordcloud basic stuff regular expressions set provide download here user informatik uni goettingen de sherbold txt line contains single tweet load any further already work problems clean using methods discussed lecture cleaned default calculate weighted inverse document change

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Text Mining</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With this exercise, you can learn more about text mining. The exercise is an extension of the example you know from the text. Instead of only eight tweets, you should analyze the complete corpus of Donald Trump tweets from 2017 and use word clouds to visualize how your preprocessing modifies the data.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Data-and-Libraries">Data and Libraries<a class="anchor-link" href="#Data-and-Libraries"> </a></h2><p>Your task in this exercise is to analyze textual data. You will perform various processing steps and see how the results of a simple visualization through word clouds evolve. You can find everything you need in the <code>nltk</code> and <code>wordcloud</code> libraries (+ some basic stuff, e.g., for regular expressions).</p>
<p>For this exercise set, we provide data about the tweets from Donald Trump in 2017. You can download the data <a href="https://user.informatik.uni-goettingen.de/~sherbold/trump-tweets-2017.txt">here</a>, each line contains a single tweet.</p>
<h2 id="Word-clouds-without-pre-processing">Word clouds without pre-processing<a class="anchor-link" href="#Word-clouds-without-pre-processing"> </a></h2><p>Load the data and create a word cloud without any further processing of the text data. Does this already work? What are problems?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Pre-processing-textual-data">Pre-processing textual data<a class="anchor-link" href="#Pre-processing-textual-data"> </a></h2><p>Clean up the textual data, e.g., using the methods discussed in the lecture. Create a new word cloud based on the cleaned corpus.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Use-TF-IDF-instead-of-TF">Use TF-IDF instead of TF<a class="anchor-link" href="#Use-TF-IDF-instead-of-TF"> </a></h2><p>The word clouds are based on simple term frequencies (TF) by default. Calculate the tf-idf, i.e., the term frequency weighted with the inverse document frequency and create a new word cloud based on these frequencies. How does it change?</p>

</div>
</div>
</div>
</div>

 


    </main>
    